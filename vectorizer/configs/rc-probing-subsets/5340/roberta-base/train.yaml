input:
  dataset: rc-probing
  input_dir: /datasets/probing/rc_probing/v0.3/balanced
  input_file_name: train5340.tsv
  task_type: sequence-level # sequence-level or token-level
  token_level_label_mode: # leave empty for sequence-level tasks {first, all, average} [teacher, ##s]
  labels_file_name: # leave empty for sequence-level tasks
  max_length: 128
  batch_size: 100

model:
  model_type: roberta-mlm # when using -mlm we have 14 layers
  model_name_or_path: roberta-base # Path to pre-trained model or shortcut name of huggingface transformer models
  config_name: # leave empty if the same name or dir path as model_name
  tokenizer_name: # leave empty if the same name or dir path as model_name
  do_lower_case: false
  cache_dir: /pre-trained-transformers
  layer: 0 # get's overwritten by args.layer if specified
  pooler: mean # get's overwritten by args.layer if specified

vectorizer:
  enable: true
  embeddings_dir: /datasets/probing/rc_probing/v0.3/balanced/embeddings/sentence-embeddings-no-padding/roberta-base

indexer:
  enable: false
  index_dir:
  min_freq: 5

labels:
  enable: true
  labels_dir: /datasets/probing/rc_probing/v0.3/balanced/embeddings/sentence-embeddings-no-padding/labels
  masks_dir: # save attention mask token-level tasks
