input:
  dataset: senteval-bigram-shift
  input_dir: /datasets/probing/senteval-bigram-shift
  input_file_name: train.tsv
  task_type: sequence-level # sequence-level or token-level
  token_level_label_mode: # leave empty for sequence-level tasks {first, all, average} [teacher, ##s]
  labels_file_name: # leave empty for sequence-level tasks
  max_length: 128
  batch_size: 100

model:
  model_type: bert-finetuned
  model_name_or_path: /checkpoints/sst-2/Mar20_09-03-28_bert-base-cased_warmup-constant # Path to pre-trained model or shortcut name of huggingface transformer models
  config_name: # leave empty if the same name or dir path as model_name
  tokenizer_name: # leave empty if the same name or dir path as model_name
  do_lower_case: false
  cache_dir: /pre-trained-transformers
  layer: 0 # get's overwritten by args.layer if specified
  pooler: cls # get's overwritten by args.layer if specified

vectorizer:
  enable: true
  embeddings_dir: /datasets/probing/senteval-bigram-shift/embeddings/sentence-embeddings/bert-base-cased-finetuned-sst-2
indexer:
  enable: false
  index_dir:
  min_freq: 5

labels:
  enable: true
  labels_dir: /datasets/probing/senteval-bigram-shift/embeddings/sentence-embeddings/labels
  masks_dir: # save attention mask token-level tasks