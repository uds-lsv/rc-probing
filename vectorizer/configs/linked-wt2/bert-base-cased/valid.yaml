input:
  dataset: linked-wt2
  input_dir: /datasets/lm-data/linked-wt2/enhanced-wt2/preprocessed/single-sentences
  input_file_name: valid_first1000.tsv
  task_type: sequence-level # sequence-level or token-level
  labels_file_name: # leave empty for sequence-level tasks
  max_length: 128
  batch_size: 100

model:
  model_type: bert
  model_name_or_path: bert-base-cased # Path to pre-trained model or shortcut name of huggingface transformer models
  config_name: # leave empty if the same name or dir path as model_name
  tokenizer_name: # leave empty if the same name or dir path as model_name
  do_lower_case: false
  cache_dir: /pre-trained-transformers
  layer: 0 # get's overwritten by args.layer if specified
  pooler: # get's overwritten by args.layer if specified

vectorizer:
  enable: false
  embeddings_dir: /datasets/lm-data/linked-wt2/enhanced-wt2/preprocessed/single-sentences/embeddings/bert-base-cased

indexer:
  enable: false
  index_dir:
  min_freq: 5

labels:
  enable: true
  labels_dir: /datasets/lm-data/linked-wt2/enhanced-wt2/preprocessed/single-sentences/embeddings/labels # token-level labels are tokenizer specific
  masks_dir: /datasets/lm-data/linked-wt2/enhanced-wt2/preprocessed/single-sentences/embeddings/labels # token-level masks are tokenizer specific
