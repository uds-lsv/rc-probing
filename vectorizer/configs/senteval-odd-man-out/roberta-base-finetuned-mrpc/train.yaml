input:
  dataset: senteval-odd-man-out
  input_dir: /datasets/probing/senteval-odd-man-out
  input_file_name: train.tsv
  task_type: sequence-level # sequence-level or token-level
  token_level_label_mode: # leave empty for sequence-level tasks {first, all, average} [teacher, ##s]
  labels_file_name: # leave empty for sequence-level tasks
  max_length: 128
  batch_size: 100

model:
  model_type: roberta-finetuned
  model_name_or_path: /checkpoints/mrpc/Mar20_08-58-15_roberta-base_warmup-constant # Path to pre-trained model or shortcut name of huggingface transformer models
  config_name: # leave empty if the same name or dir path as model_name
  tokenizer_name: # leave empty if the same name or dir path as model_name
  do_lower_case: false
  cache_dir: /pre-trained-transformers
  layer: 0 # get's overwritten by args.layer if specified
  pooler: cls # get's overwritten by args.layer if specified

vectorizer:
  enable: true
  embeddings_dir: /datasets/probing/senteval-odd-man-out/embeddings/sentence-embeddings/roberta-base-finetuned-mrpc

indexer:
  enable: false
  index_dir:
  min_freq: 5

labels:
  enable: true
  labels_dir: /datasets/probing/senteval-odd-man-out/embeddings/sentence-embeddings/labels
  masks_dir: # save attention mask token-level tasks